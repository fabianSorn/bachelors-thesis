%% LaTeX2e class for student theses
%% sections/conclusion.tex
%%
%% Karlsruhe University of Applied Sciences
%% Faculty of  Computer Science and Business Information Systems
%% Distributed Systems (vsys)
%%
%% Prof. Dr. Christian Zirpins
%% christian.zirpins@hs-karlsruhe.de
%%
%%
%% Version 0.2, 2017-11-15
%%
%% --------------------------------------------------------
%% | Derived from sdqthesis by Erik Burger burger@kit.edu |
%% --------------------------------------------------------


\chapter{Conclusion}
\label{ch:conclusion}

This chapter will provide a summary over this work as well as a outlook in the
future for further improvements.

\section{Summary}
\label{sec:Conclusion:Summary}

The purpose of this work was to analyze the performance requirements for python
plotting libraries at CERN, developing a design and implement a solution which
allows use case driven performance benchmarking.

In the beginning we had a look on all major technologies and topics involved in
the later solution to create a common knowledge base for the following chapters.
We started with a general introduction into data visualization, explored
different types of benchmarking software to learn about performance testing and
discussed the fundamental principles of the Qt application framework with
Widgets, Layouts, Signals and Slots as well as the Event System.

Following that, we investigated use cases for plotting libraries at CERN
originating from three different applications. Based on the expected update
frequencies of each use case we chose the frame rate as our metric of choice to
describe the rendering performance. A short summary of the profiling options in
Python presented a useful tooling option to discover performance bottle necks in
end-user applications.

Using the findings from the previous chapters, we further analyzed the
requirements for defining each use case and designed a solution which allowed us
to depict them. Our focus for the implementation was providing a use case
definition interface which was easy to use, capable enough to cover all
collected use cases and easily parametrizable, an user friendly \gls{cli} for
executing multiple use cases, as well as a helpful representation of the
recorded results.

To investigate, how close the results reached the later production application
the use case originated from, we compared the results of different use cases
with a minimal example application. Even tough we could measure an overhead
produced by the framework during the execution, the results could still
realistically represent performance we could expect.

\section{Outlook}
\label{sec:Conclusion:Outlook}

Based on the developed \emph{widgetmark} framework, users can conveniently
create own use cases for their own plotting needs and easily measure the
performance it reached in different plotting libraries. This allows them to
choose a fitting plotting library not only based on the offered features but
also on the performance capabilities they need.

While its use at this point is relatively narrow, there are multiple
opportunities to extend the purpose of the framework. For other \gls{gui}
Frameworks, new backends could be implemented, for more plotting libraries the
plotting abstraction layer can be extended and for other use case scenarios, the
use case interface could be extended for example with a setup and tear down
phase.  To keep widgetmark as modular as possible, it could be extended by
plug-in functionalities, which would allow extending the framework more easily
without having to change the base implementation.

Another area of improvement is minimizing the overhead added by the framework
during the execution. By minimizing this overhead the results could be even
closer to the later real life performances.

Additionally the findings in analysis and design can bring insights into other
areas of software performance evaluation and can be a template for other
benchmarking frameworks which do not focus on \gls{gui} widgets.
